{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p id='#top'>\n",
    "SparkContext is automatically stored in a variable named sc<br>\n",
    "sc.parallelize() method is the SparkContext's parallelize method to create a parallelized collection. This allows Spark\n",
    "to distribute the data across multiple nodes, instead of depending on a single node to process the data:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.parallelize(\n",
    "[('Mike', 19), ('June', 18), ('Rachel',16), ('Rob', 18), ('Scott', 17), ('Jack', 25)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li>Collect <b>action</b> will return all values in the RDD from the Spark worker nodes to the driver\n",
    "<li>A Spark driver (aka an applicationâ€™s driver process) is a JVM process that hosts SparkContext for a Spark application. It is the master node in a Spark application.\n",
    "<li>There are performance implications when working with a large amount of data as this translates to large volumes of data being transferred from the Spark worker nodes to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take(n) method returns the first n elements of the RDD instead of the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD.getNumPartitions() #returns the number of partitions the data was split into"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower bound for number of partitions is 2 x number of cores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading Data From Files</h3>\n",
    "<table align='left'><tr><td>Storage type <td>Example\n",
    "<tr><td>Local files <td>sc.textFile('/local folder/filename.csv')\n",
    "<tr><td>Hadoop HDFS <td>sc.textFile('hdfs://folder/filename.csv')\n",
    "<tr><td>S3<td>sc.textFile('s3://bucket/folder/filename.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = sc.textFile('s3://cis4567-salehan/Spark/Data/airport-codes-na.txt') \n",
    "# you may need to update this address based on file location on your system\n",
    "airports.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.getNumPartitions() # The default number for text files is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.count() #each line is one item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transformations</h3><br>\n",
    "use the <b>.map()</b> function to <b>transform</b> the data from a list of strings to a list of lists<br>\n",
    "<b>lambda:</b> An anonymous function (that is, a function defined without a name) composed of a single expression<br>\n",
    "The following code replaces each line by its list of words split by TAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = airports.map(\n",
    "    lambda line: line.split(\"\\t\"))\n",
    "airports.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sc.textFile('s3://cis4567-salehan/Spark/Data/departuredelays.csv') \n",
    "flights.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sc.textFile(\n",
    "    's3://cis4567-salehan/Spark/Data/departuredelays.csv', \n",
    "    minPartitions=8).map(\n",
    "    lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_comma (line):\n",
    "    return line.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile(\n",
    "    's3://cis4567-salehan/Spark/Data/departuredelays.csv', \n",
    "    minPartitions=8).map(split_by_comma).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map() to extract out the first two columns\n",
    "airports.map(lambda c: (c[0], c[1])).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>filter</b>(f) transformation returns a new RDD based on selecting elements for which\n",
    "the f function returns true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User filter() to filter where second column == \"WA\"\n",
    "#use \\ to create multiline statement\n",
    "\n",
    "airports\\\n",
    ".map(lambda c: (c[0], c[1]))\\\n",
    ".filter(lambda c: c[1] == \"WA\")\\\n",
    ".take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we do the thing as above without a lambda function\n",
    "def f(x):\n",
    "    if x[1] == \"CA\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "(\n",
    "airports\n",
    ".filter(f)\n",
    ".map(lambda c: (c[0], c[1]))\n",
    ".take(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>flatMap</b>(f) transformation is similar to map, but the new RDD flattens out all of the\n",
    "elements (that is, a sequence of events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only second column == \"WA\",\n",
    "# select first two columns within the RDD,\n",
    "# and flatten out all values\n",
    "(\n",
    "airports\n",
    ".filter(lambda c: c[1] == \"WA\")\n",
    ".map(lambda c: (c[0], c[1]))\n",
    ".flatMap(lambda x: x)\n",
    ".take(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>distinct()</b> transformation returns a new RDD containing the distinct elements of the\n",
    "source RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the distinct elements for the\n",
    "# third column of airports representing\n",
    "# countries\n",
    "(\n",
    "airports\n",
    ".map(lambda c: c[2])\n",
    ".distinct()\n",
    ".take(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flights.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>sample</b>(withReplacement, fraction, seed) transformation samples a fraction of the data, with or without replacement (the withReplacement parameter), based on a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a sample based on 0.001% the\n",
    "# flights RDD data specific to the fourth\n",
    "# column (origin city of flight)\n",
    "# without replacement (False) using random\n",
    "# seed of 123\n",
    "(\n",
    "flights\n",
    ".map(lambda c: c[3])\n",
    ".sample(False, 0.001, 1)\n",
    ".count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample function doesn't return the same sample size because spark internally uses something called Bernoulli sampling for taking the sample. The fraction argument doesn't represent the fraction of the actual size of the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <B>join</B>(RDD) transformation returns an RDD of (key, (val_left, val_right)) when calling\n",
    "RDD (key, val_left) and RDD (key, val_right). Outer joins are supported through left outer\n",
    "join, right outer join, and full outer join.<br>\n",
    "Join uses the first column of each record as key and the second columns of each RDD are merged into a tuple representing the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flights data (origin, date)\n",
    "# e.g. (u'JFK', u'01010900')\n",
    "flt = flights.map(lambda c: (c[3], c[0]))\n",
    "# Airports data (IATA, state)\n",
    "# e.g. (u'JFK', u'NY')\n",
    "air = airports.map(lambda c: (c[3], c[1]))\n",
    "# Execute inner join between RDDs\n",
    "flt.join(air).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>repartition</b>(n) transformation repartitions the RDD into n partitions by randomly\n",
    "reshuffling and uniformly distributing data across the network. As noted in the preceding\n",
    "recipes, this can improve performance by running more parallel threads concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The flights RDD has 8 partitions\n",
    "flights.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's re-partition this to 8 so we can have 8\n",
    "# partitions\n",
    "flights2 = flights.repartition(16)\n",
    "# Checking the number of partitions for the flights2 RDD\n",
    "flights2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>zipWithIndex</b>() transformation appends (or ZIPs) the RDD with the element indices.\n",
    "This is very handy when wanting to remove the header row (first row) of a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View each row within RDD + the index\n",
    "# i.e. output is in form ([row], idx)\n",
    "ac = airports.map(lambda c: (c[0], c[3]))\n",
    "ac.zipWithIndex().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the header from your data, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using zipWithIndex to skip header row\n",
    "# - filter out row 0\n",
    "# - extract only row info\n",
    "(\n",
    "ac\n",
    ".zipWithIndex()\n",
    ".filter(lambda row: row[1] > 0)\n",
    ".map(lambda row: row[0])\n",
    ".take(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>reduceByKey</b>(f) transformation reduces the elements of the RDD using f by the key.\n",
    "The f function should be commutative and associative so that it can be computed correctly\n",
    "in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine delays by originating city\n",
    "# - remove header row via zipWithIndex()\n",
    "# and map()\n",
    "(\n",
    "flights\n",
    ".zipWithIndex()\n",
    ".filter(lambda row: row[1] > 0)\n",
    ".map(lambda row: row[0])\n",
    ".map(lambda c: (c[3], int(c[1]))) #map to (destination, delay)\n",
    ".reduceByKey(lambda x, y: x + y) #the first element is used as key\n",
    ".take(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>sortByKey</b>(asc=True) transformation orders (key, value) RDD by key and returns an RDD in\n",
    "ascending or descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the origin code and delays, remove header\n",
    "# runs a group by origin code via reduceByKey()\n",
    "# sorting by the key (origin code)\n",
    "(\n",
    "flights\n",
    ".zipWithIndex()\n",
    ".filter(lambda row: row[1] > 0)\n",
    ".map(lambda row: row[0])\n",
    ".map(lambda c: (c[3], int(c[1])))\n",
    ".reduceByKey(lambda x, y: x + y)\n",
    ".sortByKey()\n",
    ".take(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>sortBy</b>(f, asc=True) transformation orders (key, value) RDD using the specified function 'f' and returns an RDD in\n",
    "ascending or descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "flights\n",
    ".zipWithIndex()\n",
    ".filter(lambda row: row[1] > 0)\n",
    ".map(lambda row: row[0])\n",
    ".map(lambda c: (c[3], int(c[1])))\n",
    ".reduceByKey(lambda x, y: x + y)\n",
    ".sortBy(lambda x: x[1], False)\n",
    ".take(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>union</b>(RDD) transformation returns a new RDD that is the union of the source and\n",
    "argument RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `a` RDD of Washington airports\n",
    "a = (\n",
    "airports\n",
    ".zipWithIndex()\n",
    ".filter(lambda row: row[1] > 0)\n",
    ".map(lambda row: row[0])\n",
    ".filter(lambda c: c[1] == \"WA\")\n",
    ")\n",
    "# Create `b` RDD of British Columbia airports\n",
    "b = (\n",
    "airports\n",
    ".zipWithIndex()\n",
    ".filter(lambda row: row[1] > 0)\n",
    ".map(lambda row: row[0])\n",
    ".filter(lambda c: c[1] == \"BC\")\n",
    ")\n",
    "# Union WA and BC airports\n",
    "a.union(b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>mapPartitionsWithIndex</b>(f) is similar to map but runs the f function separately on\n",
    "each partition and provides an index of the partition. It is useful to determine the data skew\n",
    "within partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/a/38957067/1100699\n",
    "def partitionElementCount(idx, iterator):\n",
    "    count = 0\n",
    "    for _ in iterator:\n",
    "        count += 1\n",
    "    return idx, count\n",
    "# Use mapPartitionsWithIndex to determine\n",
    "flights.mapPartitionsWithIndex(partitionElementCount).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformation takes an existing RDD and transforms it into one or more output RDDs. It is also a lazy process that is not initiated until an action is executed (e.g., take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flights data\n",
    "# e.g. (u'JFK', u'01010900')\n",
    "flt = flights.map(lambda c: (c[3], c[0]))\n",
    "# Airports data\n",
    "# e.g. (u'JFK', u'NY')\n",
    "air = airports.map(lambda c: (c[3], c[1]))\n",
    "# Execute inner join between RDDs\n",
    "output = flt.join(air)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check this out in spark UI. You need to have a SSH connection to your EMR master before you can connect to Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to get a list of python packages insatlled on the cluster\n",
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to insatll a package on the cluster\n",
    "sc.install_pypi_package('pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
